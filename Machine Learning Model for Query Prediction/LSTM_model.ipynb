{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation With LSTM Recurrent Neural Networks in Python with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to load the sql_logs into memory and convert all of the characters to lowercase to reduce the vocabulary the network must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  80366\n",
      "Total Vocab:  49\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = r\"C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\sql_logs.csv\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 80366 characters, and when converted to lowercase, there are only 49 distinct characters in the vocabulary for the network to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the sql_logs into sequences, converting the characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  80266\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    " seq_in = raw_text[i:i + seq_length]\n",
    " seq_out = raw_text[i + seq_length]\n",
    " dataX.append([char_to_int[char] for char in seq_in])\n",
    " dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the code to this point shows that when you split up the dataset into training data for the network to learn that you have just under 80366 training patterns. This makes sense as, excluding the first 100 characters, you have one training pattern to predict each of the remaining characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining the LSTM model. Here, defining a single hidden LSTM layer with 256 memory units. The network uses dropout a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 49 characters between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lokes\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\lokes\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\lokes\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\lokes\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "628/628 [==============================] - ETA: 0s - loss: 3.1340 - accuracy: 0.1357\n",
      "Epoch 1: loss improved from inf to 3.13400, saving model to C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-01-3.1340.hdf5\n",
      "628/628 [==============================] - 166s 260ms/step - loss: 3.1340 - accuracy: 0.1357\n",
      "Epoch 2/10\n",
      "628/628 [==============================] - ETA: 0s - loss: 2.2852 - accuracy: 0.3302\n",
      "Epoch 2: loss improved from 3.13400 to 2.28516, saving model to C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-02-2.2852.hdf5\n",
      "628/628 [==============================] - 164s 261ms/step - loss: 2.2852 - accuracy: 0.3302\n",
      "Epoch 3/10\n",
      "628/628 [==============================] - ETA: 0s - loss: 1.7391 - accuracy: 0.4871\n",
      "Epoch 3: loss improved from 2.28516 to 1.73908, saving model to C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-03-1.7391.hdf5\n",
      "628/628 [==============================] - 167s 266ms/step - loss: 1.7391 - accuracy: 0.4871\n",
      "Epoch 4/10\n",
      "628/628 [==============================] - ETA: 0s - loss: 1.0090 - accuracy: 0.7007\n",
      "Epoch 4: loss improved from 1.73908 to 1.00897, saving model to C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-04-1.0090.hdf5\n",
      "628/628 [==============================] - 185s 295ms/step - loss: 1.0090 - accuracy: 0.7007\n",
      "Epoch 5/10\n",
      "628/628 [==============================] - ETA: 0s - loss: 0.7748 - accuracy: 0.7707\n",
      "Epoch 5: loss improved from 1.00897 to 0.77481, saving model to C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-05-0.7748.hdf5\n",
      "628/628 [==============================] - 165s 263ms/step - loss: 0.7748 - accuracy: 0.7707\n",
      "Epoch 6/10\n",
      "628/628 [==============================] - ETA: 0s - loss: 0.7717 - accuracy: 0.7819\n",
      "Epoch 6: loss improved from 0.77481 to 0.77166, saving model to C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-06-0.7717.hdf5\n",
      "628/628 [==============================] - 169s 268ms/step - loss: 0.7717 - accuracy: 0.7819\n",
      "Epoch 7/10\n",
      "628/628 [==============================] - ETA: 0s - loss: 1.6890 - accuracy: 0.5082\n",
      "Epoch 7: loss did not improve from 0.77166\n",
      "628/628 [==============================] - 165s 263ms/step - loss: 1.6890 - accuracy: 0.5082\n",
      "Epoch 8/10\n",
      "628/628 [==============================] - ETA: 0s - loss: 0.5214 - accuracy: 0.8393\n",
      "Epoch 8: loss improved from 0.77166 to 0.52144, saving model to C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-08-0.5214.hdf5\n",
      "628/628 [==============================] - 165s 263ms/step - loss: 0.5214 - accuracy: 0.8393\n",
      "Epoch 9/10\n",
      "628/628 [==============================] - ETA: 0s - loss: 0.4316 - accuracy: 0.8612\n",
      "Epoch 9: loss improved from 0.52144 to 0.43157, saving model to C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-09-0.4316.hdf5\n",
      "628/628 [==============================] - 165s 263ms/step - loss: 0.4316 - accuracy: 0.8612\n",
      "Epoch 10/10\n",
      "628/628 [==============================] - ETA: 0s - loss: 0.3966 - accuracy: 0.8693\n",
      "Epoch 10: loss improved from 0.43157 to 0.39657, saving model to C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-10-0.3966.hdf5\n",
      "628/628 [==============================] - 168s 267ms/step - loss: 0.3966 - accuracy: 0.8693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a8ce2ffc70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "# define the checkpoint\n",
    "filepath=r\"C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "# Here, a modest number of 10 epochs and a large batch size of 128 patterns\n",
    "model.fit(X, y, epochs=10, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy of model is accuracy: 0.8693"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After running the code, the hdf5 file is generated of weight checkpoint files and save in the given directory ('C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights') path file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text using the trained LSTM network is relatively straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the saved hdf5 file and copy the path and paste in filename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = r\"C:\\Users\\lokes\\OneDrive\\Desktop\\optacloud_assignment_lokesh naik\\ML_model\\weights\\weights-improvement-10-0.3966.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM model to making predictions first start with a seed sequence as input, generate the next characters, then updating the seed sequence to add the generated characters on the end and trim off the first characters. This process is repeated for as long as we want to predict new query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" mit 1\"\n",
      "show index from `classicmodels`.`customers`\n",
      "delete from offices where city = 56 limit 1\n",
      "delet \"\n",
      "e from productl where productcode = 23 limit 1\n",
      "\"uelect ordernumber, orddrdate, requireddate, shippeddate, status from orders limit 1\"\n",
      "\"update orders set ordernumber = 90, orderdate = 50, requireddate = 27, shippeddate = 82, status = 17 where shiupeddate = 25 limit 1\"\n",
      "\"update orders set ordernumber = 90, orderdate = 50, requireddate = 27, shippeddate = 82, status = 17 where shiupeddate = 25 limit 1\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(400):\n",
    " x = np.reshape(pattern, (1, len(pattern), 1))\n",
    " x = x / float(n_vocab)\n",
    " prediction = model.predict(x, verbose=0)\n",
    " index = np.argmax(prediction)\n",
    " result = int_to_char[index]\n",
    " seq_in = [int_to_char[value] for value in pattern]\n",
    " sys.stdout.write(result)\n",
    " pattern.append(index)\n",
    " pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The generated text with the random seed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
